{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LSTM Assignement John Isak Villanger\n",
    "For this assignement I used the meditations by Marcus Aurelius as data.\n",
    "Here is some of the generated wisdom:\n",
    "\n",
    "![Alt text](https://imagizer.imageshack.com/img922/5897/nYX4lW.jpg?raw=true \"Title\")\n",
    "\n",
    "![Alt text](https://imagizer.imageshack.com/img923/888/I5yxZo.jpg?raw=true \"Title\")\n",
    "\n",
    "![Alt text](https://imagizer.imageshack.com/img922/6212/ehyEsS.jpg?raw=true \"Title\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Alices Adventures in Wonderland, by Lewis Carroll \n",
      " \n",
      "This eBook is for the use of anyone anywhere in the United States and \n",
      "most other parts of the world at no cost and with almost no restrictions \n",
      "whatsoever. You may copy it, give it away or reuse it under the terms \n",
      "of the Project Gutenberg License included with this eBook or online at \n",
      "www.gutenberg.org. If you are not located in the United States, you \n",
      "will have to check the laws of the country where you are located before \n",
      "using this eBook. \n",
      " \n",
      "Title Alices Adventures in Wonderland \n",
      " \n",
      "Author Lewis Carroll \n",
      " \n",
      "Release Date January, [eBook ] \n",
      "[Most recently updated October , ] \n",
      " \n",
      "Language English \n",
      " \n",
      "Character set encoding UTF \n",
      " \n",
      "Produced by Arthur DiBianca and David Widger \n",
      " \n",
      " START OF THE PROJECT GUTENBERG EBOOK ALICES ADVENTURES IN WONDERLAND \n",
      " \n",
      "[Illustration] \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Alices Adventures in Wonderland \n",
      " \n",
      "by Lewis Carroll \n",
      " \n",
      "THE MILLENNIUM FULCRUM EDITION \n",
      " \n",
      "Contents \n",
      " \n",
      " CHAPTER I. Down the RabbitHole \n",
      " \n",
      "total chars:  62\n",
      "['\\n', ' ', ',', '.', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 62782192000 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-5c7bf44c9940>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;31m# One Hot encoding of data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxlen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchars\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlong\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msentence\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 62782192000 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import requests\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#with urlopen('https://www.gutenberg.org/files/11/11-0.txt') as file:\n",
    "#    text = file.read()\n",
    "text = \"\"\n",
    "for i in range(11, 100):\n",
    "    response = requests.get(\"https://www.gutenberg.org/files/\" + str(i) + \"/\" + str(i) + \"-0.txt\")\n",
    "    response = response.text\n",
    "    text += response\n",
    "#print(text[0:1000])\n",
    "text = re.sub(\"\\d\\.\",\"\", text) # Replace the leading before each chapter\n",
    "text = re.sub(\"[\\t\\r]\",\" \", text) # Replace newlines\n",
    "text = re.sub(\"[^A-z,. \\n]\",\"\", text) # Remove everything that's not a normal char ,. or space\n",
    "text = re.sub(\"  *\",\" \", text) # Replace instances of two or more spaces with one\n",
    "\n",
    "print(text[0:1000])\n",
    "#Code taken from: https://towardsdatascience.com/generating-text-using-a-recurrent-neural-network-1c3bfee27a5e\n",
    "chars = sorted(list(set(text))) # Getting all unique chars\n",
    "print('total chars: ', len(chars))\n",
    "print(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "#print(text)\n",
    "\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "\n",
    "# One Hot encoding of data\n",
    "x = torch.zeros((len(sentences), maxlen, len(chars)), dtype=torch.float32)\n",
    "y = torch.zeros((len(sentences)), dtype=torch.long)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i] = char_indices[next_chars[i]]\n",
    "\n",
    "\n",
    "torch.save(x,\"100booksX.pt\")\n",
    "#print(x)\n",
    "torch.save(y,\"100booksY.pt\")\n",
    "#x = torch.load(\"x.pt\")\n",
    "#y = torch.load(\"y.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataSet(torch.utils.data.Dataset):\n",
    "  def __init__(self, X, Y):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "    if len(self.X) != len(self.Y):\n",
    "      raise Exception(\"The length of X does not match the length of Y\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # note that this isn't randomly selecting. It's a simple get a single item that represents an x and y\n",
    "    _x = self.X[index]\n",
    "    _y = self.Y[index]\n",
    "\n",
    "    return _x, _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-b2a23c16cbff>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDataSet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m73500\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1022\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgenerator\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mGenerator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmanual_seed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m42\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mtrain\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataLoader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m256\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataLoader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m256\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "data = DataSet(x, y)\n",
    "print(len(data))\n",
    "train, val = torch.utils.data.random_split(data, [73500, 1022], generator=torch.Generator().manual_seed(42))\n",
    "train = torch.utils.data.DataLoader(train, 256, shuffle=True)\n",
    "val = torch.utils.data.DataLoader(val, 256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, n_hidden, n_layers):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(len(chars), n_hidden, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(n_hidden, len(chars))\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x, hidden):   # -> input x:    (batch_size, seq_length, num_features)\n",
    "        x, hidden = self.lstm(x, hidden) # -> LSTM out:   (batch_size, seq_length, hidden_size)\n",
    "        x = x[:,-1]  # -> Taking last output from each sequence.\n",
    "        x = self.linear(x)\n",
    "        return x, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = LSTMPredictor(n_hidden=512, n_layers=2)\n",
    "model = model.to(device)\n",
    "train_size = 73000\n",
    "val_size = 1164\n",
    "adam = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Will automatically restore weights from epoch with best validation accuracy\n",
    "def train_model(model, loss_f, optimizer, num_epochs):\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_model_acc = 0.0\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    train_acc = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"epoch\",epoch+1)\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        torch.set_grad_enabled(True)\n",
    "        best_acc = 0\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train):\n",
    "            model.zero_grad()\n",
    "            #print(batch.shape)\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            #Forward\n",
    "            out, hidden = model(inputs, None )\n",
    "            #print(out)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            #print(preds)\n",
    "            #print(type(preds[0]))\n",
    "\n",
    "            #Compute objective function\n",
    "            #print(out)\n",
    "            #print(labels)\n",
    "            loss = loss_f(out, labels)\n",
    "\n",
    "            #Clean the gradients\n",
    "            #model.zero_grad()\n",
    "\n",
    "            #Accumulate partial deriviates wrt parameters\n",
    "            loss.backward()\n",
    "\n",
    "            #Step in the opposite direction og the gradient wrt optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            #stats\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            #print(preds)\n",
    "            #print(labels.data)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss[epoch] = running_loss / train_size\n",
    "        train_acc[epoch] = running_corrects.double() / train_size\n",
    "        print('Training Loss: {:.4f} Acc: {:.4f}'.format(train_loss[epoch], train_acc[epoch]))\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for batch_idx, batch in enumerate(val):\n",
    "            model.eval()\n",
    "            inputs, labels = batch\n",
    "\n",
    "            #Move to gpu if availible\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            #Forward\n",
    "            with(torch.no_grad()):\n",
    "                out, hidden = model(inputs, None)\n",
    "            _, preds = torch.max(out,1)\n",
    "\n",
    "            #Compute objective function\n",
    "            loss = loss_f(out, labels)\n",
    "\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "        val_loss[epoch] = running_loss / val_size\n",
    "        val_acc[epoch] = running_corrects.double() / val_size\n",
    "\n",
    "        print('Validation Loss: {:.4f} Acc: {:.4f}'.format(val_loss[epoch], val_acc[epoch]))\n",
    "\n",
    "        # deep copy the model\n",
    "        if (val_acc[epoch] > best_acc):\n",
    "            best_acc = val_acc[epoch]\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_acc, train_loss, val_acc, val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model, train_acc, train_loss, val_acc, val_loss = train_model(model, cross_entropy, adam, 15)\n",
    "torch.save(model, \"model\")\n",
    "#Plotting the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_acc, label=\"Training\")\n",
    "ax.plot(val_acc, label=\"Validation\")\n",
    "ax.set_xticklabels(np.arange(len(train_acc)))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training-validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss, label=\"Training\")\n",
    "ax.plot(val_loss, label=\"Validation\")\n",
    "ax.set_xticklabels(np.arange(len(train_loss)))\n",
    "plt.xlabel(\"Epoch-1\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training-validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model = torch.load(\"modeltest\")\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "def Probabilistic_selection(out):\n",
    "    out = F.F.softmax(out, 1)\n",
    "    selection = np.random.uniform()\n",
    "    rolling_sum = 0\n",
    "    index = 0\n",
    "    while True:\n",
    "            rolling_sum += out[0][index]\n",
    "            if rolling_sum > selection:\n",
    "                break\n",
    "            index += 1\n",
    "    preds =[index]\n",
    "    return preds\n",
    "\n",
    "for j in range(1):\n",
    "    seed = \"The purpose of life\"\n",
    "    answer = \"\"\n",
    "\n",
    "    #Generates a random seed of 3 letters\n",
    "    #for k in range(3):\n",
    "    #    seed += chr(np.random.randint(97, 122))\n",
    "    x = torch.zeros((len(seed), len(chars)), dtype=torch.float32)\n",
    "    for i in range(len(seed)):\n",
    "        x[i, char_indices[seed[i]]] = 1\n",
    "\n",
    "    hidden = None\n",
    "    x = x.view(-1, len(x), len(x[0]))\n",
    "    x = x.to(device)\n",
    "    for i in range(1000):\n",
    "\n",
    "        out, hidden = model(x, hidden)\n",
    "        #preds = Probabilistic_selection(out) #Selects letters probabilistically from a softmax activation\n",
    "        _, preds = torch.max(out,1) #Selects letters deterministically\n",
    "        char = indices_char[int(preds[0])]\n",
    "        answer += char\n",
    "        x = torch.zeros((1,1,len(chars)), dtype=torch.float32, device=device)\n",
    "        x[0,0,int(preds[0])] = 1\n",
    "\n",
    "    print(\"\\n\" + answer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}