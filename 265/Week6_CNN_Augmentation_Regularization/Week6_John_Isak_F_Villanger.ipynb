{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Week 6\n",
    "#### John Isak F. Villanger.\n",
    "#### Questions:\n",
    "2. Do you expect the classification to be as easy as the MNIST-dataset?\n",
    "The classification here seems to be harder than MNIST, generally numbers can be recognized by recognizing lines and\n",
    "circles and then knowing the relations between lines and circles constitue each number. Here you will just get some blob,\n",
    "and the blob for each thing will have different shape depending on the angle.\n",
    "\n",
    "5. Visualize overfitting?\n",
    "You can see train validation accuracy/loss for the two models at the bottom.\n",
    "Training accuracy seems to steadily improve across the epochs, while validation accuracy seems to plateau around 63%.\n",
    "You can see some signs of overfitting in the loss graph, where it starts to increase for the validation data.\n",
    "\n",
    "7. Were the strategies successful in coping with overfitting?\n",
    "I use L2 regularization on Adam by adjusting the weight-decay parameter(set it to 0.001). From the pytorch documentation,\n",
    "this is an L2 regularization optimized for Adam. For data augmentation, i flipped the image horizontally with a p=0.5.\n",
    "I also randomly erased some of the pixels in the image, this seems to be equivalent to using dropout in the input layer.\n",
    "These strategies where definitely successful in improving the models performance on the validation data, you also see\n",
    "fewer signs of overfitting on the loss graph for the validation data, where it is steadily decreasing across all the epochs.\n",
    "It's imporant to note that you can't compare the training-val relation between the strategies being included or not,\n",
    "since the image augmentation will reduce performance on the training data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transf\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def data_augmentation():\n",
    "    augmentations = transf.Compose([\n",
    "        transf.RandomHorizontalFlip(),\n",
    "        transf.RandomErasing(),\n",
    "    ])\n",
    "    return augmentations\n",
    "\n",
    "\n",
    "def show_images(trainloader, classes):\n",
    "    dataiter = iter(trainloader)\n",
    "    images_b, labels_b = dataiter.next()\n",
    "    for idx, image in enumerate(images_b):\n",
    "        plt.imshow(images_b[idx].numpy().transpose(1, 2, 0))\n",
    "        plt.title(classes[labels_b[idx]])\n",
    "        plt.show()\n",
    "\n",
    "def make_model():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5), #28x28\n",
    "        nn.MaxPool2d(2, stride=2, padding=0), #14x14\n",
    "        nn.Conv2d(12, 24, 3), #12x12\n",
    "        nn.MaxPool2d(2, stride=2, padding=0), #6x6\n",
    "        nn.Conv2d(24, 42, 3), #4x4\n",
    "        nn.MaxPool2d(2, stride=2, padding=0), #2x2\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(168, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 10)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_model_2():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5), #28x28\n",
    "        nn.MaxPool2d(2, stride=2, padding=0), #14x14\n",
    "        nn.Conv2d(12, 24, 3), #12x12\n",
    "        nn.MaxPool2d(2, stride=2, padding=0), #6x6\n",
    "        nn.Conv2d(24, 42, 3), #4x4\n",
    "        nn.MaxPool2d(2, stride=2, padding=0), #2x2\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(168, 64),\n",
    "        nn.ReLU(),\n",
    "        # nn.Dropout(0.2),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        # nn.Dropout(0.1),\n",
    "        nn.Linear(32, 10)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    images = torchvision.datasets.CIFAR10(\"CIFAR10\", train=True, transform=transf.ToTensor(), download=False)\n",
    "    train, val = torch.utils.data.random_split(images, [45000, 5000])\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val, batch_size=1)\n",
    "\n",
    "    show_images(train_loader, images.classes)\n",
    "\n",
    "    model = make_model()\n",
    "    adam = torch.optim.Adam(model.parameters())\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    model, train_acc, train_loss, val_acc, val_loss = train_model(model,cross_entropy, adam, 30, train_loader,\n",
    "                                                                  val_loader, None)\n",
    "    make_plots(train_acc, train_loss, val_acc, val_loss)\n",
    "\n",
    "    model_2 = make_model_2()\n",
    "    adam_2 = torch.optim.Adam(model_2.parameters(), weight_decay=0.001)\n",
    "    model_2, train_acc_2, train_loss_2, val_acc_2, val_loss_2 = train_model(model_2, cross_entropy, adam_2, 30,\n",
    "                                                                            train_loader, val_loader, data_augmentation())\n",
    "    make_plots(train_acc_2, train_loss_2, val_acc_2, val_loss_2)\n",
    "\n",
    "\n",
    "\n",
    "def make_plots(train_acc, train_loss, val_acc, val_loss):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.arange(1,len(train_acc)+1),train_acc, label=\"Training\")\n",
    "    ax.plot(np.arange(1,len(val_acc)+1),val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training-validation accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.arange(1,len(train_loss)+1), train_loss, label=\"Training\")\n",
    "    ax.plot(np.arange(1,len(val_loss)+1),val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training-validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, loss_f, optimizer, num_epochs, train, val, augmentations):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    train_size = len(train.dataset)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    train_acc = np.zeros(num_epochs)\n",
    "    val_size = len(val.dataset)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"epoch\",epoch+1)\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        torch.set_grad_enabled(True)\n",
    "        for batch_idx, batch in enumerate(train):\n",
    "            model.train()\n",
    "            inputs, labels = batch\n",
    "            if (augmentations != None):\n",
    "                inputs = augmentations(inputs)\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "\n",
    "            #Forward\n",
    "            out = model(inputs)\n",
    "            _, preds = torch.max(out, 1)\n",
    "\n",
    "            #Compute objective function\n",
    "            loss = loss_f(out, labels)\n",
    "\n",
    "            #Clean the gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            #Accumulate partial deriviates wrt parameters\n",
    "            loss.backward()\n",
    "\n",
    "            #Step in the opposite direction og the gradient wrt optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            #stats\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss[epoch] = running_loss / train_size\n",
    "        train_acc[epoch] = running_corrects / train_size\n",
    "        print('Training Loss: {:.4f} Acc: {:.4f}'.format(train_loss[epoch], train_acc[epoch]))\n",
    "        torch.set_grad_enabled(False)\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        #torch.set_grad_enabled(False)\n",
    "        for batch_idx, batch in enumerate(val):\n",
    "            model.eval()\n",
    "            inputs, labels = batch\n",
    "\n",
    "            #Move to gpu if availible\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "\n",
    "            #Forward\n",
    "            out = model(inputs)\n",
    "            _, preds = torch.max(out,1)\n",
    "\n",
    "            #Compute objective function\n",
    "            loss = loss_f(out, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss[epoch] = running_loss / val_size\n",
    "        val_acc[epoch] = running_corrects / val_size\n",
    "\n",
    "        print('Validation Loss: {:.4f} Acc: {:.4f}'.format(val_loss[epoch], val_acc[epoch]))\n",
    "\n",
    "        # deep copy the model\n",
    "        if (val_acc[epoch] > best_acc):\n",
    "            best_acc = val_acc[epoch]\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    return model, train_acc, train_loss, val_acc, val_loss\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}