{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Model selection of a Ridge model on a list of hyper-parameters instances with KFold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Boston dataset:\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "Y = boston.target\n",
    "# Display scatterplots of target prices with respect to each of the 13 features:\n",
    "colors = ['cornflowerblue',\n",
    "          'tab:orange',\n",
    "          'tab:green',\n",
    "          'r',\n",
    "          'tab:purple',\n",
    "          'tab:brown',\n",
    "          'tab:pink',\n",
    "          'b',\n",
    "          'tab:olive',\n",
    "          'tab:cyan',\n",
    "          'lightcoral',\n",
    "          'chocolate',\n",
    "          'springgreen']\n",
    "for col in range(X.shape[1]):\n",
    "    plt.figure(1, figsize=(24, 15))\n",
    "    if col < X.shape[1] - 1:\n",
    "        plot_idx = col+1\n",
    "    else:\n",
    "        plot_idx = 14\n",
    "    plt.subplot(5, 3, plot_idx)\n",
    "    plt.scatter(X[:,col], Y, marker='o', c=colors[col])\n",
    "    plt.xlabel(boston.feature_names[col])\n",
    "    plt.ylabel('Target price')\n",
    "plt.suptitle(\"Target prices with respect to each of the 13 features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target prices appear to be strongly correlated to several continuous features... among them, features 5 (\"RM\") and 12 (\"LSTAT\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.scatter(X[:,5], X[:,12], c='k')\n",
    "plt.title(\"Visualizing correlation of features 5 & 12\")\n",
    "plt.xlabel(boston.feature_names[5])\n",
    "plt.ylabel(boston.feature_names[12])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features 5 and 12 appear to be visually sufficiently decorrelated. Let us choose these two features as a subset of the full features matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,[5,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select a Ridge model on a list of hyper-parameters instances, via regular validation:\n",
    "def regular_model_selection(X, Y, hyper_parameters_instances, seed):\n",
    "    # Extract a test set:\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=seed)\n",
    "    # Extract a validation set:\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.2, shuffle=True, random_state=seed)\n",
    "    # For each hyper-parameter instance, do regular validation:\n",
    "    val_MSEs = []\n",
    "    for hyper_parameters_instance in hyper_parameters_instances:\n",
    "        print(\"\\nNow preprocessing hyper-parameter instance\", hyper_parameters_instance)\n",
    "        val_MSE = assess_Ridge(X_train=X_train, \n",
    "                               X_eval=X_val,\n",
    "                               Y_train=Y_train,\n",
    "                               Y_eval=Y_val,\n",
    "                               degree=hyper_parameters_instance[\"degree\"],\n",
    "                               regularization=hyper_parameters_instance[\"regularization\"])\n",
    "        print(\"Validation MSE:\", val_MSE)\n",
    "        val_MSEs.append(val_MSE)\n",
    "    # The hyper-parameter instance with the smallest validation MSE is our model of choice:\n",
    "    best_instance_idx = np.argmin(np.array(val_MSEs))\n",
    "    best_hyper_parameters_instance = hyper_parameters_instances[best_instance_idx]\n",
    "    print(\"\\n\\nBest hyper-parameter instance:\", best_hyper_parameters_instance)\n",
    "    # Train the best model on the whole train set then evaluate it on the test set:\n",
    "    best_model_test_MSE = assess_Ridge(X_train=X_train_val, \n",
    "                                       X_eval=X_test,\n",
    "                                       Y_train=Y_train_val,\n",
    "                                       Y_eval=Y_test,\n",
    "                                       degree=best_hyper_parameters_instance[\"degree\"],\n",
    "                                       regularization=best_hyper_parameters_instance[\"regularization\"])\n",
    "    print(\"Test MSE:\", best_model_test_MSE)\n",
    "\n",
    "### Split+shuffle X and Y into k=num_folds different folds:\n",
    "def KFold_split(X, Y, num_folds, seed):\n",
    "    KFold_splitter = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "    X_train_folds = []\n",
    "    X_val_folds = []\n",
    "    Y_train_folds = []\n",
    "    Y_val_folds = []\n",
    "    for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X, Y):\n",
    "        X_train_folds.append(X[kth_fold_train_idxs])\n",
    "        X_val_folds.append(X[kth_fold_val_idxs])\n",
    "        Y_train_folds.append(Y[kth_fold_train_idxs])\n",
    "        Y_val_folds.append(Y[kth_fold_val_idxs])\n",
    "    return X_train_folds, X_val_folds, Y_train_folds, Y_val_folds\n",
    "\n",
    "### Select a Ridge model on a list of hyper-parameters instances, via Kfold cross-validation:\n",
    "def KFold_model_selection(X, Y, hyper_parameters_instances, num_folds, seed):\n",
    "    # Extract a test set:\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=seed)\n",
    "    # Extract train and validation folds:\n",
    "    X_train_folds, X_val_folds, Y_train_folds, Y_val_folds = KFold_split(X_train_val, Y_train_val, num_folds, seed)\n",
    "    # For each hyper-parameter instance, do KFold cross validation:\n",
    "    mean_val_MSEs = []\n",
    "    for hyper_parameters_instance in hyper_parameters_instances:\n",
    "        print(\"\\nNow preprocessing hyper-parameter instance\", hyper_parameters_instance)\n",
    "        mean_val_MSE = perform_KFold_CV(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds, \n",
    "                                        degree=hyper_parameters_instance[\"degree\"], \n",
    "                                        regularization=hyper_parameters_instance[\"regularization\"])\n",
    "        print(\"Mean validation MSE:\", mean_val_MSE)\n",
    "        mean_val_MSEs.append(mean_val_MSE)\n",
    "    # The hyper-parameter instance with the smallest mean validation MSE is our model of choice:\n",
    "    best_instance_idx = np.argmin(np.array(mean_val_MSEs))\n",
    "    best_hyper_parameters_instance = hyper_parameters_instances[best_instance_idx]\n",
    "    print(\"\\n\\nBest hyper-parameter instance:\", best_hyper_parameters_instance)\n",
    "    # Train the best model on the whole train set then evaluate it on the test set:\n",
    "    best_model_test_MSE = assess_Ridge(X_train=X_train_val, \n",
    "                                       X_eval=X_test,\n",
    "                                       Y_train=Y_train_val,\n",
    "                                       Y_eval=Y_test,\n",
    "                                       degree=best_hyper_parameters_instance[\"degree\"],\n",
    "                                       regularization=best_hyper_parameters_instance[\"regularization\"])\n",
    "    print(\"Test MSE:\", best_model_test_MSE)\n",
    "    \n",
    "### KFold cross-validation of a Ridge model with given hyper-parameters:\n",
    "def perform_KFold_CV(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds, degree, regularization):\n",
    "    val_fold_MSEs = []\n",
    "    # For each fold, assess a surrogate model with fixed hyper-parameters:\n",
    "    cmpt = 0\n",
    "    for X_train_fold, X_val_fold, Y_train_fold, Y_val_fold in zip(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds):\n",
    "        val_fold_MSE = assess_Ridge(X_train=X_train_fold, \n",
    "                                    X_eval=X_val_fold,\n",
    "                                    Y_train=Y_train_fold,\n",
    "                                    Y_eval=Y_val_fold,\n",
    "                                    degree=degree,\n",
    "                                    regularization=regularization)\n",
    "        cmpt += 1\n",
    "        print(\"Surrogate model\", str(cmpt) + \"/\" + str(len(X_val_folds)), \"validation MSE:\", val_fold_MSE)\n",
    "        val_fold_MSEs.append(val_fold_MSE)\n",
    "    # Compute the mean validation MSE between all the folds:\n",
    "    mean_val_MSE = np.mean(np.array(val_fold_MSEs))\n",
    "    return mean_val_MSE\n",
    "\n",
    "### Fit and evaluate a Ridge model with given hyper-parameters:\n",
    "def assess_Ridge(X_train, X_eval, Y_train, Y_eval, degree, regularization):\n",
    "    # Build the polynomial features:\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_eval_poly = poly_features.fit_transform(X_eval)\n",
    "    # Fit the polynomial features with a Ridge model:\n",
    "    model = Ridge(alpha=regularization)\n",
    "    model.fit(X_train_poly, Y_train)\n",
    "    # Evaluate the Ridge model on the evaluation set:\n",
    "    Y_eval_pred = model.predict(X_eval_poly)\n",
    "    eval_MSE = mean_squared_error(Y_eval, Y_eval_pred)\n",
    "    return eval_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of hyper-parameters instances:\n",
    "hyper_parameters_instances = [{\"degree\": 1, \"regularization\": 0},\n",
    "                              {\"degree\": 2, \"regularization\": 0},\n",
    "                              {\"degree\": 3, \"regularization\": 0},\n",
    "                              {\"degree\": 4, \"regularization\": 0},\n",
    "                              {\"degree\": 1, \"regularization\": 0.001},\n",
    "                              {\"degree\": 2, \"regularization\": 0.001},\n",
    "                              {\"degree\": 3, \"regularization\": 0.001},\n",
    "                              {\"degree\": 4, \"regularization\": 0.001},\n",
    "                              {\"degree\": 1, \"regularization\": 0.01},\n",
    "                              {\"degree\": 2, \"regularization\": 0.01},\n",
    "                              {\"degree\": 3, \"regularization\": 0.01},\n",
    "                              {\"degree\": 4, \"regularization\": 0.01},\n",
    "                              {\"degree\": 1, \"regularization\": 0.1},\n",
    "                              {\"degree\": 2, \"regularization\": 0.1},\n",
    "                              {\"degree\": 3, \"regularization\": 0.1},\n",
    "                              {\"degree\": 4, \"regularization\": 0.1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model selection of a Ridge model on a list of hyper-parameters instances with regular validation:\n",
    "# Fix random seed for reproducibility:\n",
    "#seed = 333\n",
    "# Or not:\n",
    "seed = None\n",
    "# Select model with regular validation:\n",
    "regular_model_selection(X, Y, hyper_parameters_instances, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model selection of a Ridge model on a list of hyper-parameters instances with KFold cross-validation:\n",
    "# Fix random seed for reproducibility:\n",
    "#seed = 333\n",
    "# Or not:\n",
    "seed = None\n",
    "# Select model with KFold cross-validation:\n",
    "KFold_model_selection(X, Y, hyper_parameters_instances, num_folds=5, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A singular matrix warning indicates that the matrix containing the data has columns that are approximately linearly dependant, thus the matrix becomes close to singular. \n",
    "\n",
    "Increasing the regularization hyper-parameter will eventually get rid of this warning. Indeed, recall that the Ridge model solves $\\underset{w}{\\min} \\|y-Xw\\|_2^2 + \\alpha\\|w\\|_2^2$ by nullifying the gradient, which yields a unique solution given by $w_{sol} = (X^tX + \\alpha I)^{-1} X^ty$. The higher the $\\alpha$ in $X^tX + \\alpha I$, the more dominant the identity matrix term becomes compared to the potentially singular matrix term $X^tX$. When the identity matrix is dominant enough the matrix $X^tX + \\alpha I$ becomes invertible i.e. non-singular. \n",
    "\n",
    "Intuitively, the penalization term $\\alpha\\|w\\|_2^2$ forces the model to shrink the minimizer $w_{sol}$ toward $0$ (the bigger $\\alpha$ is, the stronger the shrinkage), which lowers the impact from irrelevant variables in the model and makes the model more resilient to columns (features) colinearity in the data matrix.\n",
    "\n",
    "In the context of this exercice, with a Ridge model of degree $3$ we may have to push the regularization hyper-parameter at a very high value ($>100$) to get rid of the singular matrix warning, and we then may get an ill-conditioned matrix warning instead. To get rid of this new warning, the regularization hyper-parameter should be set even higher.\n",
    "\n",
    "Notice that if you run the model selections several time, the cross-validation tends to be more stable than the regular validation in the sense that certain hyperparameters instances are selected quite often with cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
