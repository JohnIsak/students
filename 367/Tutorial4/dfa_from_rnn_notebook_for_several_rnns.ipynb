{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome!\n",
    "\n",
    "This is another notebook demonstration the extraction method from our [ICML 2018 paper](https://arxiv.org/abs/1711.09576), only with a little extra functionality to help you keep track of multiple networks, and without documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from LSTM import LSTMNetwork\n",
    "from GRU import GRUNetwork\n",
    "from RNNClassifier import RNNClassifier\n",
    "from Training_Functions import mixed_curriculum_train\n",
    "from Tomita_Grammars import tomita_1, tomita_2, tomita_3, tomita_4, tomita_5, tomita_6, tomita_7\n",
    "from Training_Functions import make_train_set_for_target\n",
    "from Extraction import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    None == all_my_rnns\n",
    "except NameError:\n",
    "    all_my_rnns = []\n",
    "    \n",
    "class Wrapper:\n",
    "    def __init__(self,rnn,alphabet,target,name,train_set):\n",
    "        self.rnn = rnn\n",
    "        self.alphabet = alphabet\n",
    "        self.target = target\n",
    "        self.name = name\n",
    "        words = sorted(list(train_set.keys()),key=lambda x:len(x))\n",
    "        short_pos = next((w for w in words if target(w)==True),None)\n",
    "        short_neg = next((w for w in words if target(w)==False),None)\n",
    "        self.starting_examples = [w for w in [short_pos,short_neg] if not None==w]\n",
    "        self.train_set = train_set\n",
    "        self.dfas = []\n",
    "    def __repr__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"01\"\n",
    "target = tomita_3\n",
    "name = \"my Tomita 3 RNN\"\n",
    "\n",
    "# alternative option (example):\n",
    "# def target(w):\n",
    "#     if len(w)==0:\n",
    "#         return True\n",
    "#     return w[0]==w[-1]\n",
    "# alphabet = \"abc\"\n",
    "# name = \"start and end same over abc\"\n",
    "\n",
    "\n",
    "rnn = RNNClassifier(alphabet,num_layers=1,hidden_dim=10,RNNClass = LSTMNetwork)\n",
    "\n",
    "\n",
    "train_set = make_train_set_for_target(target,alphabet)\n",
    "wrapper = Wrapper(rnn,alphabet,target,name,train_set)\n",
    "all_my_rnns.append(wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mixed_curriculum_train(wrapper.rnn,wrapper.train_set,stop_threshold = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrapper.rnn.renew()  \n",
    "# you only really need this if you start messing about and doing weird stuff. \n",
    "# It cleans the computation graph, but doesn't reset the weights so don't worry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wrapper.dfas.append(\n",
    "    extract(wrapper.rnn,time_limit=50,initial_split_depth=10,starting_examples=wrapper.starting_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = all_my_rnns[-1]\n",
    "wrapper.rnn.renew()\n",
    "dfa_index = -1 # if you want to check the DFA from the last time you extracted something for this network\n",
    "dfa = wrapper.dfas[-1]\n",
    "test_set = wrapper.train_set \n",
    "print(\"testing on train set, i.e. test set is train set\")\n",
    "# here we're printing things on the train set, but you can also try other sets if you like\n",
    "# for instance you could also make one by running make_train_set_for_target(wrapper.target,wrapper.alphabet)\n",
    "\n",
    "\n",
    "from math import pow\n",
    "def percent(num,digits=2):\n",
    "    tens = pow(10,digits)\n",
    "    return str(int(100*num*tens)/tens)+\"%\"\n",
    "\n",
    "dfa.draw_nicely(maximum=30) #max size willing to draw\n",
    "n = len(test_set)\n",
    "print(\"current test set size:\", n)\n",
    "pos = len([w for w in test_set if wrapper.target(w)])\n",
    "print(\"of which positive:\",pos,\"(\"+percent(pos/n)+\")\")\n",
    "rnn_target = len([w for w in test_set if wrapper.rnn.classify_word(w)==wrapper.target(w)])\n",
    "print(\"rnn score against target on test set:\",rnn_target,\"(\"+percent(rnn_target/n)+\")\")\n",
    "dfa_rnn = len([w for w in test_set if wrapper.rnn.classify_word(w)==dfa.classify_word(w)])\n",
    "print(\"extracted dfa score against rnn on test set:\",dfa_rnn,\"(\"+percent(dfa_rnn/n)+\")\")\n",
    "dfa_target = len([w for w in test_set if dfa.classify_word(w)==wrapper.target(w)])\n",
    "print(\"extracted dfa score against target on test set:\",dfa_target,\"(\"+percent(dfa_target/n)+\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-4dd591b4",
   "language": "python",
   "display_name": "PyCharm (Oblig3-367)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}