{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Obligatory Exercise 2\n",
    "##### John Isak F. Villanger\n",
    "\n",
    "#### 1\n",
    "- $k $ number of samples\n",
    "- $2n $ number of literals\n",
    "- $\\epsilon $ maximal accepted error rate\n",
    "- $\\delta$ maximal accepted failure rate of $\\epsilon$\n",
    "\n",
    "The algorithm runs for $k$ iterations, generating an example and updates these variables:\n",
    "- $Z_-$ The number of negative examples\n",
    "- $Z_0(l): l \\in literals$  The number of times the literal has been evaluated to 0 in the examples.\n",
    "- $Z_{0+} (l): l \\in literals$  The number of times the literal has been evaluated to 0 in the positive examples.\n",
    "\n",
    "next the algorithm generates a hypothesis:\n",
    "\n",
    "$$\\frac{Z_0(l)}{k} \\geq \\frac{\\epsilon}{64n^2}$$\n",
    "\n",
    "where any literal over the threshold is included in the hypothesis. validity this term\n",
    "is given in lemma 2 p363 of the learning from noisy examples paper.\n",
    "We can see that literals are included here if they \"rarely arise\" from the probability distribution.\n",
    "The intuition behind this is that there is little need to include positive literals that almost exclusively get\n",
    "evaluated to 1, another way to put it: the formula $a\\land b$ and the formula $b$ would be equivalent if $a$ always was true.\n",
    "\n",
    "next the algorithm tries to remove from the hypothesis the literals that are often evaluated to 0 in positive examples.\n",
    "by this threshold.\n",
    "\n",
    "$$\\frac{Z_{0+}}{Z_{0}} > \\eta'+ \\frac{\\epsilon(1-2\\eta_b)}{8n}$$\n",
    "\n",
    "where $\\eta'$ is an estimate of noise and $\\eta_b$ is an upper bound of noise.\n",
    "the validity of this term is given in lemma 7 p365 of the learning from noisy examples paper.\n",
    "\n",
    "\n",
    "#### 2\n",
    "An hypothesis class $H$ can shatter $N$ data points if we can find a hypothesis $h \\in H$ that separates the positive examples from the negative for every problem.\n",
    "VC dimension of an hypothesis class $H$ is the maximum number of data points that can be shattered by $H$\n",
    "\n",
    "$VC(F) = \\infty $. Since $h$ and $t$ are the same type(CNF Formula), we can always set $h = t$.\n",
    "and this $h$ will separate every positive example from the negative examples.\n",
    "\n",
    "If we assume $H$ is finite then $VC(F)\\leq log_2\\vert H\\vert$.\n",
    "\n",
    "Assume $VC(F)> log_2\\vert H\\vert$ and $VC(F) = n$\n",
    "then there are $2^n$ possible ways to classify the data points as positive or negative.\n",
    "$2^n > \\vert H\\vert \\to log_2 2^n > log_2 \\vert H\\vert \\to n > log_2 \\vert H\\vert \\to VC(F)> log_2\\vert H\\vert$\n",
    "Since $2^n > \\vert H\\vert$ By the reverse pigeonhole principle there must be a classification that the hypothesis space can't capture.\n",
    " The assumption $VC(F)> log_2\\vert H\\vert$ is incorrect.\n",
    "so by contradiction $VC(F) \\leq log_2\\vert H\\vert$\n",
    "\n",
    "#### Some of my observations regarding the algorithm\n",
    "The \"width\" of the distribution greatly seems to impact how well the algorithm works.\n",
    "Wider distribution in this implementation will mean more often picking positive examples, while narrower will mean more\n",
    "often picking negative examples. I think this means that it's hardest for the algorithm when positive examples are picked at a slightly rate than $\\epsilon$,\n",
    "since the hypothesis will very likely end up being inconsistent, but the error rate of an inconsistent hypothesis will be too high\n",
    "to satisfy $\\epsilon$. I find this problem arising with $sd = 15$ and the value of $k$ doesn't seem to impact it much, tested for $k = 10 000$.\n",
    "I wonder if there is a mistake in my implementation causing this or if this is the type of scenario where are really high $k$ is needed.\n",
    "\n",
    "I did not estimate $\\eta_b$, the method presented in learning from noisy examples established a definitive upper bound,\n",
    "but the runtime seems very high, but they also introduced more practical methods for estimating noise later on in the paper.\n",
    "I believe a practical way to estimate $\\eta_b$ could be to run the algorithm a set number of times, and pick the highest $\\eta'$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing with k = 1\n",
      "Accuracy: 0.625\n",
      "Accuracy: 0.75\n",
      "Accuracy: 0.0\n",
      "Accuracy: 0.125\n",
      "Accuracy: 0.4\n",
      "Accuracy: 0.825\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.35\n",
      "Accuracy: 0.75\n",
      "Accuracy: 0.8\n",
      "Accuracy: 0.925\n",
      "Accuracy: 0.625\n",
      "Accuracy: 0.375\n",
      "Accuracy: 0.725\n",
      "Accuracy: 0.8\n",
      "Accuracy: 0.6\n",
      "Accuracy: 0.4\n",
      "Accuracy: 0.5\n",
      "Accuracy: 0.575\n",
      "Accuracy: 0.875\n",
      "Accuracy: 0.675\n",
      "Accuracy: 0.475\n",
      "Accuracy: 0.75\n",
      "Accuracy: 0.6\n",
      "Accuracy: 0.375\n",
      "Accuracy: 0.975\n",
      "Accuracy: 0.25\n",
      "Accuracy: 0.9\n",
      "Accuracy: 0.625\n",
      "Accuracy: 0.125\n",
      "Accuracy: 0.9\n",
      "Accuracy: 0.75\n",
      "Accuracy: 1.0\n",
      "More than 10 higher than epsilon? 27\n",
      "testing with k = 10\n",
      "Accuracy: 0.9\n",
      "Accuracy: 0.75\n",
      "Accuracy: 0.5825\n",
      "Accuracy: 0.765\n",
      "Accuracy: 0.8525\n",
      "Accuracy: 0.955\n",
      "Accuracy: 0.9675\n",
      "Accuracy: 0.895\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.94\n",
      "Accuracy: 0.7675\n",
      "Accuracy: 0.725\n",
      "Accuracy: 0.88\n",
      "Accuracy: 0.8675\n",
      "Accuracy: 0.8125\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.955\n",
      "Accuracy: 0.6725\n",
      "Accuracy: 0.9725\n",
      "Accuracy: 0.92\n",
      "Accuracy: 0.77\n",
      "Accuracy: 0.9775\n",
      "Accuracy: 0.685\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.82\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.8075\n",
      "Accuracy: 0.87\n",
      "Accuracy: 0.8225\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.71\n",
      "Accuracy: 0.92\n",
      "More than 10 higher than epsilon? 18\n",
      "testing with k = 100\n",
      "Accuracy: 0.975\n",
      "Accuracy: 0.95825\n",
      "Accuracy: 0.91975\n",
      "Accuracy: 0.57825\n",
      "Accuracy: 0.88875\n",
      "Accuracy: 0.8985\n",
      "Accuracy: 0.93025\n",
      "Accuracy: 0.754\n",
      "Accuracy: 0.62125\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.925\n",
      "Accuracy: 0.968\n",
      "Accuracy: 0.60675\n",
      "Accuracy: 0.976\n",
      "Accuracy: 0.8895\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.95725\n",
      "Accuracy: 0.74625\n",
      "Accuracy: 0.88125\n",
      "Accuracy: 0.6745\n",
      "Accuracy: 0.69525\n",
      "Accuracy: 0.944\n",
      "Accuracy: 0.967\n",
      "Accuracy: 0.95325\n",
      "Accuracy: 0.69325\n",
      "Accuracy: 0.60525\n",
      "Accuracy: 0.64025\n",
      "Accuracy: 0.95025\n",
      "Accuracy: 0.57625\n",
      "Accuracy: 0.89225\n",
      "Accuracy: 0.61\n",
      "Accuracy: 0.53625\n",
      "Accuracy: 0.65525\n",
      "More than 10 higher than epsilon? 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Literal:\n",
    "    def __init__(self, value, name):\n",
    "        self.value = value\n",
    "        self.name = name\n",
    "        self.in_target = False\n",
    "        self.in_hypothesis = False\n",
    "        self.negation = None\n",
    "        #Number of times a literal is evaluated as 0 in an example\n",
    "        self.z0 = 0\n",
    "        #Number of times a literal is evaluated as 0 in a positive example\n",
    "        self.z0_pos = 0\n",
    "        #z0_pos / z0\n",
    "        self.h = 0\n",
    "    def __str__(self):\n",
    "        return str(self.name) + \"  \" + str(self.value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.name) + \" \" + str(self.value)+\", \"\n",
    "\n",
    "\n",
    "def generate_literals(n):\n",
    "    literals = np.empty(2*n, dtype=Literal)\n",
    "    for i in range(n):\n",
    "        literals[2*i] = Literal(True, i)\n",
    "        literals[2*i+1] = Literal(False, i)\n",
    "        literals[2*i].negation = literals[2*i+1]\n",
    "        literals[2*i+1].negation = literals[2*i]\n",
    "        #print(literals[2*i])\n",
    "        #print(literals[2*i+1])\n",
    "    return literals\n",
    "\n",
    "\n",
    "def generate_target(length, literals, distribution, noise, n):\n",
    "    target = np.empty(length, dtype=Literal)\n",
    "    for i in range(length):\n",
    "        contradicting_literal = True\n",
    "        while contradicting_literal:\n",
    "            index = random.randint(0, n-1)\n",
    "            value = distribution[index] + random.randint(-noise, noise)\n",
    "            if not literals[2*index].in_target and not literals[2*index+1].in_target:\n",
    "                if value < 50:\n",
    "                    target[i] = literals[2*index+1]\n",
    "                    target[i].in_target = True\n",
    "                    contradicting_literal = False\n",
    "                else:\n",
    "                    target[i] = literals[2*index]\n",
    "                    target[i].in_target = True\n",
    "                    contradicting_literal = False\n",
    "    return sorted(target, key= lambda x: x.name)\n",
    "\n",
    "\n",
    "def generate_example(literals, distribution, noise, n, mean):\n",
    "    example = np.empty(n, dtype=Literal)\n",
    "    for i in range (0, n):\n",
    "        value = distribution[i] + random.randint(-noise, noise)\n",
    "        if value < mean:\n",
    "            example[i] = literals[2*i+1]\n",
    "        else:\n",
    "            example[i] = literals[2*i]\n",
    "    return example\n",
    "\n",
    "\n",
    "def update_h(literals):\n",
    "    for lit in literals:\n",
    "        lit.h = lit.z0_pos/lit.z0 if lit.z0 != 0 else 1\n",
    "\n",
    "\n",
    "def generate_distribution(size, mean, sd):\n",
    "    #distribution = np.random.binomial(100, 0.5, size)\n",
    "    distribution = np.random.normal(mean, sd, size)\n",
    "    return distribution\n",
    "\n",
    "\n",
    "def positive_example(example, noise_label):\n",
    "    pos_example = True\n",
    "    for i in range(example.size):\n",
    "        if example[i].negation.in_target:\n",
    "            pos_example = False\n",
    "    val = random.random()\n",
    "    return pos_example if val >= noise_label else not pos_example\n",
    "\n",
    "\n",
    "def update_literals(example, label_example):\n",
    "    for i in range(example.size):\n",
    "        example[i].negation.z0 += 1\n",
    "        if label_example:\n",
    "            example[i].negation.z0_pos += 1\n",
    "\n",
    "\n",
    "def update_hypothesis(literals, n_prime, sb):\n",
    "    for literal in literals:\n",
    "        if literal.in_hypothesis and literal.h > n_prime+sb/2:\n",
    "            literal.in_hypothesis = False\n",
    "\n",
    "\n",
    "def print_hypothesis(literals):\n",
    "    for literal in literals:\n",
    "        if literal.in_hypothesis:\n",
    "            print(literal)\n",
    "\n",
    "\n",
    "#Returns fraction of correct guesses by hypothesis\n",
    "def estimate_accuracy(n_error_estimate, literals, distribution, noise, n, mean):\n",
    "    correct_guesses = 0\n",
    "    for i in range(n_error_estimate):\n",
    "        example = generate_example(literals, distribution, noise, n, mean)\n",
    "        correct_guess = True\n",
    "        correct_guess_n = False\n",
    "        pos_ex = positive_example(example, 0)\n",
    "        for j in range(example.size):\n",
    "            if example[j].negation.in_hypothesis:\n",
    "                correct_guess = False\n",
    "                correct_guess_n = True\n",
    "        if (correct_guess and pos_ex) or (correct_guess_n and not pos_ex):\n",
    "            correct_guesses += 1\n",
    "    return correct_guesses/n_error_estimate\n",
    "\n",
    "def generate_hypothesis(literals, k, q_i):\n",
    "    for lit in literals:\n",
    "        if lit.z0/k >= q_i/2:\n",
    "            lit.in_hypothesis = True\n",
    "\n",
    "def find_n2(literals):\n",
    "    min_noise = 1\n",
    "    for lit in literals:\n",
    "        if lit.h is not None and lit.h < min_noise:\n",
    "            min_noise = lit.h\n",
    "    return min_noise\n",
    "\n",
    "def train(distribution, literals, mean, n, noise_label, noise_literal, q_i, req_size, sb, z_neg):\n",
    "    for j in range (req_size):\n",
    "            example = generate_example(literals, distribution, noise_literal, n, mean)\n",
    "            label_example = positive_example(example, noise_label)\n",
    "            if not label_example:\n",
    "                z_neg += 1\n",
    "            update_literals(example, label_example)\n",
    "    update_h(literals)\n",
    "    n1 = z_neg/req_size\n",
    "    generate_hypothesis(literals, req_size, q_i)\n",
    "    n2 = find_n2(literals)\n",
    "    #print(\"n1:\",n1,\"n2:\",n2)\n",
    "    n_prime = min(n1,n2)\n",
    "    #print(\"Noise estimated to:\", n_prime)\n",
    "    #print(\"Hypothesis before final stage\")\n",
    "    #print_hypothesis(literals)\n",
    "    #print(\"Hypothesis after final stage\")\n",
    "    update_hypothesis(literals, n_prime, sb)\n",
    "    #print_hypothesis(literals)\n",
    "\n",
    "\n",
    "#It picks a number from the Normal Distribution and assigns it to a literal\n",
    "#If the number is more than the mean the probability of picking the positive literal increases in examples and target\n",
    "#Noise is added so it will be a probability(instead of certainty) based on the normal distribution weather or not it picks the positive or negative literal\n",
    "#Thus higher SD will mean less \"noise\"\n",
    "\n",
    "def main():\n",
    "    #I recommend using 50 as mean, and instead tweaking noise or SD in order to generate different types of data\n",
    "    n = 10\n",
    "    epsilon = 0.1\n",
    "    delta = 0.3\n",
    "    mean = 50\n",
    "    sd = 15\n",
    "    noise_literal = 10\n",
    "    noise_label = 0.1\n",
    "    target_size = 5\n",
    "    error_greater_than_epsilon_count = 0\n",
    "\n",
    "    noise_ub = 0.1\n",
    "\n",
    "    k = 1\n",
    "    q_h = epsilon / 2*(2*n)\n",
    "    sb = q_h * (1-2*noise_ub)\n",
    "    q_i = q_h/(8*(2*n))\n",
    "    for i in range(3):\n",
    "        print(\"testing with k =\", k)\n",
    "        for j in range (int(10/delta)):\n",
    "            z_neg = 0\n",
    "            literals = generate_literals(n)\n",
    "            distribution = generate_distribution(n, mean, sd)\n",
    "            #print(distribution)\n",
    "            target = generate_target(target_size, literals,distribution, noise_literal, n)\n",
    "\n",
    "\n",
    "            #req_size = int(2/(epsilon*(np.log(2)+np.log(1/delta))))\n",
    "            #print(\"Required Size: \", req_size)\n",
    "            #print(10/delta)\n",
    "            #print(\"Distribution:\\n\",distribution)\n",
    "            #print(\"Target:\\n\", target)\n",
    "            train(distribution,literals,mean,n,noise_label,noise_literal,q_i,k,sb,z_neg)\n",
    "            estimated_accuracy = estimate_accuracy(k*40, literals, distribution, noise_literal, n, mean)\n",
    "            print(\"Accuracy:\", estimated_accuracy)\n",
    "            if (1 - estimated_accuracy) > epsilon:\n",
    "                error_greater_than_epsilon_count += 1\n",
    "\n",
    "        print(\"More than 10 higher than epsilon?\", error_greater_than_epsilon_count)\n",
    "        k *= 10\n",
    "        error_greater_than_epsilon_count = 0\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}